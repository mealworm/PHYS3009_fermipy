{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS3009: MeV/GeV Astronomy Project\n",
    "\n",
    "Name: \n",
    "\n",
    "Student No.:\n",
    "\n",
    "total marks: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you will do yourself the data analysis of the object \n",
    "PKS 2155-304. You mainly need to copy and paste code from the PG 1553+113 notebook which we have discussed in the lecture, and make the necessary changes for the new source. You will be provided with example output so that you can check your results. Please put your code in the cells starting with \n",
    "\n",
    "```\n",
    "# your code here\n",
    "```\n",
    "\n",
    "You will also need to answer some questions for marks. Edit the text within the cells, either providing descriptions of labeled parameters, or typing your answer where it says \"Answer here\".\n",
    "\n",
    "Do not change the code in any other cells. If you want to add additional ouput, or test some things you can create new cells for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Analysis with fermipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python likelihood tools are a very powerful set of analysis tools that expand upon the command line tools provided with the Fermi Science Tools package. Not only can you perform all of the same likelihood analysis with the python tools that you can with the standard command line tools but you can directly access all of the model parameters. You can more easily script a standard analysis. There are also a few things built into the python tools that are not available from the command line like the calculation of upper limits.\n",
    "\n",
    "There are many user contributed packages built upon the python backbone of the Science Tools and this thread will highlight the use of the [fermipy](http://fermipy.readthedocs.org) package.\n",
    "\n",
    "This sample analysis of PKS 2155-304 is based on the PG 1553+113 analysis performed by the LAT team and described in [Abdo, A. A. et al. 2010, ApJ, 708, 1310](http://adsabs.harvard.edu/abs/2010ApJ...708.1310A) and closely follows the [Likelihood Analysis with Python](http://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/python_tutorial.html) thread.  This tutorial assumes you have the most recent ScienceTools installed and [fermipy](http://fermipy.readthedocs.org) installed on top of it.  For instructions on installing fermipy and the Fermi ScienceTools you should consult the [fermipy Installation Instructions](http://fermipy.readthedocs.org/en/latest/install.html).  We will also make significant use of python, so you might want to familiarize yourself with python including matplotlib and other libraries (there's a beginner's guide at http://wiki.python.org/moin/BeginnersGuide). This tutorial also assumes that you've gone through the non-python based [Unbinned Likelihood Tutorial](http://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/likelihood_tutorial.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this thread the original data were extracted from the [LAT data server](http://fermi.gsfc.nasa.gov/cgi-bin/ssc/LAT/LATDataQuery.cgi) with the following selections (these selections are similar to those in the paper):\n",
    "\n",
    "* Search Center (RA,Dec) = (329.717,-30.2256)\n",
    "* Radius = 15 degrees\n",
    "* Start Time (MET) = 241401601 seconds (2008-08-26T00:00:00)\n",
    "* Stop Time (MET) = 257385601 seconds (2009-02-26T23:59:59)\n",
    "* Minimum Energy = 100 MeV\n",
    "* Maximum Energy = 300000 MeV\n",
    "\n",
    "The data are already in the repository in the ```data``` directory, where the output of the analysis will also go in this example.\n",
    "\n",
    "However, we will now download the diffuse emission model files, which are rather large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isdir('data'):\n",
    "    os.environ['DATADIR'] = 'data'\n",
    "\n",
    "if os.path.isfile('diffuse.tar.gz'):\n",
    "    !tar xzf diffuse.tar.gz\n",
    "else:\n",
    "    !curl -OL https://raw.githubusercontent.com/fermiPy/fermipy-extras/master/data/diffuse.tar.gz\n",
    "    !tar xzf diffuse.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a file list\n",
    "\n",
    "You'll then need to make a file list with the names of your input event files. You can either just make one with a text editor or do the following from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 data/*PH*.fits > data/PKS2155.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a config file\n",
    "\n",
    "fermipy bases its analysis on a configuration file (in [yaml](http://yaml.org) format).  We're just going to use a really simple config file for a standard analysis.  There are many many more options which you can use or you can modify these options after the fact within the analysis chain.\n",
    "\n",
    "\n",
    "Here are the contents of the included 'config.yaml' file.  For more details on the config file see [config.html](http://fermipy.readthedocs.org/en/latest/config.html).\n",
    "\n",
    "```\n",
    "data:\n",
    "  evfile : PKS2155.lst\n",
    "  scfile : L211025154921445A2D9929_SC00.fits\n",
    "  ltcube : ltcube_00.fits\n",
    "\n",
    "binning:\n",
    "  roiwidth   : 10.0\n",
    "  binsz      : 0.1\n",
    "  binsperdec : 4\n",
    "\n",
    "selection :\n",
    "  emin : 100\n",
    "  emax : 300000\n",
    "  tmin : 241401601\n",
    "  tmax : 257385601\n",
    "  zmax    : 90\n",
    "  evclass : 128\n",
    "  evtype  : 3\n",
    "  target : 'PKS 2155-304'\n",
    "\n",
    "gtlike:\n",
    "  edisp : True\n",
    "  irfs : 'P8R2_SOURCE_V6'\n",
    "  edisp_disable : ['isodiff','galdiff']\n",
    "\n",
    "model:\n",
    "  src_roiwidth : 15.0\n",
    "  galdiff  : '../diffuse/gll_iem_v07.fits'\n",
    "  isodiff  : '../diffuse/iso_P8R2_SOURCE_V6_v06.txt'\n",
    "  catalogs : ['4FGL']  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the analysis\n",
    "\n",
    "Next, you create an analysis script and run the setup steps which include running the selections and generating exposure maps etc.  This will take a bit.\n",
    "\n",
    "This is where the magic happens.  fermipy will load the point source model, create your xml file for you, decide on all the appropriate cuts and binnings and just go.  All of this is configurable from python or from the config file.  And, if you need to rerun things, it's smart enough to not overwrite files if it doesn't need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up some useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the GTAnalysis module from fermipy\n",
    "\n",
    "You start by importing the module and then creating an instance of the analysis object from our config file.  When instantiating the analysis object we can override any options defined in the configuration file by passing keyword arguments to the object constructor.  Here we explicitly set the verbosity parameter to 3 (INFO) which supresses DEBUG output.  When we create the object, it spits out a bunch of information about all of the parameters that were used.  You can see there are many more options than the ones we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fermipy.gtanalysis import GTAnalysis\n",
    "gta = GTAnalysis('data/config.yaml',logging={'verbosity': 3})\n",
    "matplotlib.interactive(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The setup routine\n",
    "\n",
    "This gets everything ready for the likelihood analysis including instantiating the pylikelihood object.  Note that fermipy will skip generating any ancillary files that already exist in the working directory.  In the sample tarball these files have already been produced in order to speed up this stage of the analysis.  If you want to see the behavior of fermipy when running from an empty working directory you can delete one or more of these files before running *setup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the analysis we'll have a quick look at the files that are produced by the setup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/*.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide a brief explanation of the contents of each file and its role in the analysis: **[7 marks]**\n",
    "\n",
    "* **ft1_00.fits**: \n",
    "* **bexpmap_00.fits**: \n",
    "* **bexpmap_roi_00.fits**: \n",
    "* **ccube_00.fits**: \n",
    "* **ltcube_00.fits**: \n",
    "* **srcmap_00.fits**: \n",
    "\n",
    "To see example of one of these files we can open and plot the counts cube file.  This is a 3D cube that contains the distribution of events as a function of energy and two spatial coordinates.  In the example below we sum over the energy dimension of the cube to make a 2-D sky image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as pyfits\n",
    "\n",
    "h = pyfits.open('data/ccube.fits')\n",
    "h.info()\n",
    "counts = h[0].data\n",
    "counts.shape\n",
    "plt.figure()\n",
    "plt.imshow(np.sum(counts,axis=0),interpolation='nearest',origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the state of the ROI prior with the print_roi() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional details about an individual source can be retrieved by printing the corresponding source object.  Here we use the bracket operator to return the properties of PKS 2155-403. The catalog name of PKS 2155-403 is '4FGL J2158.8-3013'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the likelihood fitting\n",
    "\n",
    "Now that all of the ancillary files have been generated, we can move on to the actual fitting.  The first thing you should do is free some of the sources since all of the sources are initially fixed.  We'll just free those sources in the center region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Normalization of all Sources within 3 deg of ROI center\n",
    "# your code here\n",
    "\n",
    "# Free all parameters of isotropic and galactic diffuse components\n",
    "# your code here (two lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[3 marks]**\n",
    "\n",
    "What is the purpose of freeing the normalization of sources within 3 degrees? **[1 mark]**\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple anlaysis we are leaving the spectral shapes of sources fixed but we're going to free the spectral shape of the source we care about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, actually do the fit.  The software does its best to get the fit to converge by running the fit several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary returned by the fit method returns a variety of diagnostic information about the fit including the fit quality, the relative improvement in the likelihood, and the correlations among the fit parameters.  We can inspect the results of the fit by printing the source object for PKS 2155-403."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fit Quality: ',fit_results['fit_quality'])\n",
    "print(gta.roi['4FGL J2158.8-3013'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the meaning of the following parameters: **[7 marks]**\n",
    "\n",
    "\n",
    "* **TS**             : \n",
    "* **Npred**          : \n",
    "* **Flux**           : \n",
    "* **EnergyFlux**     : \n",
    "* **b'norm'**        :  \n",
    "* **b'alpha'**       :       \n",
    "* **b'beta'**        :    \n",
    "* **b'Eb'**          :\n",
    "\n",
    "\n",
    "You can then save the state of the roi to an output file for reference later.  The write_roi function does this.  The first argument is a string that will be prepended to the names of the output files generated by this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**\n",
    "\n",
    "How many catalog point sources are there within the ROI? **[1 mark]**\n",
    "\n",
    "Answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of diagnostic plots also saved at the same time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l data/*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = glob('data/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for png in pngs:\n",
    "    my_image = Image(png)\n",
    "    display(my_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a short description of each of the figures **[4 marks]**\n",
    "\n",
    "Answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the results\n",
    "\n",
    "Since the results are saved, you can load them back up at any point (you can also get to these within python).  Here we retrieve the analysis results from the output numpy file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.load('data/fit0.npy', allow_pickle=True).flat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sources` dictionary has an entry for each source in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(c['sources'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the flux, spectral parameters, and TS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sources']['4FGL J2158.8-3013']['flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c['sources']['4FGL J2158.8-3013']['param_names'][:4])\n",
    "print(c['sources']['4FGL J2158.8-3013']['param_values'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sources']['4FGL J2158.8-3013']['ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SED is in there as well.  We can plot it.\n",
    "\n",
    "What does SED stand for **[1 mark]**\n",
    "\n",
    "Answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array(c['sources']['4FGL J2158.8-3013']['model_flux']['energies'])\n",
    "dnde = np.array(c['sources']['4FGL J2158.8-3013']['model_flux']['dnde'])\n",
    "dnde_hi = np.array(c['sources']['4FGL J2158.8-3013']['model_flux']['dnde_hi'])\n",
    "dnde_lo = np.array(c['sources']['4FGL J2158.8-3013']['model_flux']['dnde_lo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(E, (E**2)*dnde, 'k--')\n",
    "plt.loglog(E, (E**2)*dnde_hi, 'k')\n",
    "plt.loglog(E, (E**2)*dnde_lo, 'k')\n",
    "plt.xlabel('E [MeV]')\n",
    "plt.ylabel(r'E$^2$ dN/dE [MeV cm$^{-2}$ s$^{-1}$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want SED points, there's a function for that.  There are lots of options for this which you can set in the config file or from keyword arguments of the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the state to the yaml file or you can just access it directly.  This is also the way to get at the dictionary for any individual source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = gta.roi['4FGL J2158.8-3013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.loglog(E, (E**2)*dnde, 'k--')\n",
    "plt.loglog(E, (E**2)*dnde_hi, 'k')\n",
    "plt.loglog(E, (E**2)*dnde_lo, 'k')\n",
    "plt.errorbar(np.array(sed['e_ctr']),\n",
    "             sed['e2dnde'], \n",
    "             yerr=sed['e2dnde_err'], fmt ='o')\n",
    "plt.xlabel('E [MeV]')\n",
    "plt.ylabel(r'E$^{2}$ dN/dE [MeV cm$^{-2}$ s$^{-1}$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "sed_tab = Table.read('data/4fgl_j2158.8-3013_sed.fits')\n",
    "sed_tab.write('data/4fgl_j2158.8-3013_sed.ecsv',exclude_names=['norm_scan','dloglike_scan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This is it. You have done an analysis of the data recorded with Fermi-LAT on PKS 2155-304."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "Before you submit your work you should make a few checks that everything works fine.\n",
    "\n",
    "1. Save your notebook as a PDF (File->Download As->PDF). This document will help you debugging in the next step.\n",
    "1. Restart the kernel and rerun the entire notebook (Kernel->Restart & Run All). This will delete all variables (but not your code) and rerun the notebook in one go. If this does not go through the end (i.e. you do not see the output of the next cell) then you have to fix it. You will see at which cell the run stopped. A common mistake is using a variable that is defined only at a later stage.\n",
    "1. You think you fixed everything? Redo step 2 (Kernel->Restart & Run All)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a\\bYa\\boa\\bua\\b a\\baa\\bra\\bea\\b a\\bra\\bea\\baa\\bda\\bya\\b a\\bta\\boa\\b a\\bsa\\bua\\bba\\bma\\bia\\bta\\b.a\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the output of the last cell without explicitly running it? Then the notebook goes through with one kernel restart. You can proceed to submission.\n",
    "You do not see the output? Go back to step 2 above.\n",
    "\n",
    "The jupyter notebook goes through all cells with one Kernel Restart & Run all.    **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "You have to download and submit 3 files.\n",
    "- PDF file. File->Download As->PDF. Save this file on your disk. If this doesn't work you can also try Download As -> HTML. This file serves as reference for what you have done.\n",
    "- Jupyter notebook. File->Download As->Notebook (.ipynb). Save this file on your disk. This file will be tested and you receive your marks for that.\n",
    "- Spectral data points. Go to the Home Page of your jupyter notebooks. It is in a different tab or window of your browser. Find the file `data/4fgl_j2158.8-3013_sed.ecsv` and save it to your disk. You will need this file at a later stage.\n",
    "\n",
    "Please submit all 3 files on Sakai.        **[1 mark]**\n",
    "\n",
    "Congratulations. You have succesfully completed your MeV/GeV Astronomy project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "22bb53b43b3afb0848b0afa27adb8f2415fb61399be88b9ea69bd0b84fe58c05"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
